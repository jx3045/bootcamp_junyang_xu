{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 14: Deployment & Monitoring\n",
    "\n",
    "Use this template to draft your reflection and (optionally) sketch a dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reflection (200–300 words)\n",
    "- Risks if deployed:\n",
    "- Monitoring metrics across layers (Data/Model/System/Business):\n",
    "- Ownership & handoffs:\n",
    "\n",
    "> Tip: Be specific (e.g., 'p95 latency > 250ms triggers on-call notification')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e824f98",
   "metadata": {},
   "source": [
    "# Stage 14: Deployment & Monitoring — Reflection\n",
    "\n",
    "Our current project model, trained on processed financial transaction data (including features such as region, age, income, and transaction counts), would face several risks if deployed into production. On the **data layer**, schema drift could occur if upstream pipelines change column names or types. Missing values may increase unexpectedly, especially for income or spend fields, which would degrade predictions. Delayed data ingestion could also reduce freshness and cause business reports to lag.  \n",
    "\n",
    "On the **model layer**, the regression model may suffer from concept drift if user behavior or income distributions shift. For example, a 5% increase in Population Stability Index (PSI) on key features like income should trigger retraining. Rolling Mean Absolute Error (MAE) is another critical metric; if 7-day MAE rises 10% above baseline, we would investigate calibration and retrain as needed.  \n",
    "\n",
    "On the **system layer**, we must ensure API reliability and responsiveness. A p95 latency above 250ms or an error rate greater than 2% should immediately notify the platform on-call team. Batch job success rate should remain above 98%.  \n",
    "\n",
    "On the **business layer**, monitoring KPIs such as approval rate (target > 60%) and bad rate (target < 10%) ensures alignment with financial outcomes. If these deviate, analysts should review and escalate.  \n",
    "\n",
    "**Ownership** is divided: Data Engineering handles schema and null checks, ML Engineering monitors MAE and drift, Platform Ops ensures system SLAs, and Product Analysts track business KPIs. Retraining is triggered either bi-weekly or when PSI > 0.05. All issues are logged in Jira with clear escalation paths, and rollbacks are approved by the ML lead.  \n",
    "\n",
    "This layered monitoring plan provides clear thresholds, ownership, and escalation to ensure that the model remains reliable and valuable after deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Optional: Dashboard Sketch\n",
    "Describe panels and key charts. You can also attach an image file in your repo (png/pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional helper: simple structure to list metrics\n",
    "\n",
    "monitoring = {\n",
    "    \"data\": [\"freshness_minutes\", \"null_rate\", \"schema_hash\"],\n",
    "    \"model\": [\"rolling_mae\", \"calibration_error\", \"psi_drift\"],\n",
    "    \"system\": [\"p95_latency_ms\", \"error_rate\", \"batch_success_rate\"],\n",
    "    \"business\": [\"approval_rate\", \"bad_rate\", \"roi\"]\n",
    "}\n",
    "\n",
    "print(\"Stage14 Deployment & Monitoring reflection drafted.\")\n",
    "print(monitoring)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

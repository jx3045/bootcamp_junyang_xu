{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 15: Orchestration & System Design\n",
    "Complete the sections below. Keep your answers concise and focused on orchestration readiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Project Task Decomposition\n",
    "List 4–8 tasks. Add more rows as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import argparse, logging, sys, pickle\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "tasks = pd.DataFrame({\n",
    "    'task': [\n",
    "        'ingest',\n",
    "        'clean',\n",
    "        'feature_engineer',\n",
    "        'train_model',\n",
    "        'evaluate',\n",
    "        'report'\n",
    "    ],\n",
    "    'inputs': [\n",
    "        '/data/raw/*.csv',\n",
    "        'raw_ingested.csv',\n",
    "        'clean.csv',\n",
    "        'features.csv',\n",
    "        'features.csv + model.pkl',\n",
    "        'metrics.json + model.pkl'\n",
    "    ],\n",
    "    'outputs': [\n",
    "        'raw_ingested.csv',\n",
    "        'clean.csv',\n",
    "        'features.csv',\n",
    "        'model.pkl',\n",
    "        'metrics.json',\n",
    "        'report.md'\n",
    "    ],\n",
    "    'idempotent': [True, True, True, True, True, True]\n",
    "})\n",
    "\n",
    "print(\"=== Task Decomposition ===\")\n",
    "print(tasks)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(input_path: str, output_path: str, log_path: str) -> None:\n",
    "    logging.info(\"[train_model] start\")\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Fill missing values with median\n",
    "    df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "    X = df[['income', 'transactions']].values\n",
    "    y = df['spend'].values\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    logging.info(\"[train_model] MAE = %.4f\", mae)\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    pickle.dump(model, open(output_path, \"wb\"))\n",
    "    logging.info(\"[train_model] model saved at %s\", output_path)\n",
    "\n",
    "    Path(log_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(f\"MAE: {mae:.4f}\\n\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Train model task\")\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Processed feature CSV\")\n",
    "    parser.add_argument(\"--output\", required=True, help=\"Path to save model.pkl\")\n",
    "    parser.add_argument(\"--log\", required=True, help=\"Path to save metrics log\")\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "    train_model(args.input, args.output, args.log)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example CLI call (for testing)\n",
    "    main([\n",
    "        \"--input\", \"../data/processed/features.csv\",\n",
    "        \"--output\", \"../models/model.pkl\",\n",
    "        \"--log\", \"../reports/metrics.txt\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "tasks = pd.DataFrame({\n",
    "    'task': ['ingest', 'clean', 'train_or_score', 'report'],\n",
    "    'inputs': ['/data/raw.ext', 'prices_raw.json', 'prices_clean.json', 'model.json'],\n",
    "    'outputs': ['prices_raw.json', 'prices_clean.json', 'model.json', 'report.txt'],\n",
    "    'idempotent': [True, True, True, True]\n",
    "})\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dependencies (DAG)\n",
    "Describe dependencies and paste a small diagram if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = {\n",
    "    'ingest': [],\n",
    "    'clean': ['ingest'],\n",
    "    'feature_engineer': ['clean'],\n",
    "    'train_model': ['feature_engineer'],\n",
    "    'evaluate': ['feature_engineer', 'train_model'],\n",
    "    'report': ['evaluate']\n",
    "}\n",
    "\n",
    "print(\"\\n=== DAG Dependencies ===\")\n",
    "print(dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Logging & Checkpoints Plan\n",
    "Specify what you will log and where you will checkpoint for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_plan = pd.DataFrame({\n",
    "    'task': ['ingest', 'clean', 'feature_engineer', 'train_model', 'evaluate', 'report'],\n",
    "    'log_messages': [\n",
    "        'start/end, row counts, source URI',\n",
    "        'start/end, null rates before/after',\n",
    "        'new feature stats, drift checks',\n",
    "        'hyperparams, training MAE',\n",
    "        'eval metrics (MAE, calibration)',\n",
    "        'report generated path'\n",
    "    ],\n",
    "    'checkpoint_artifact': [\n",
    "        'raw_ingested.csv',\n",
    "        'clean.csv',\n",
    "        'features.csv',\n",
    "        'model.pkl',\n",
    "        'metrics.json',\n",
    "        'report.md'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Logging & Checkpoints ===\")\n",
    "print(logging_plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Right-Sizing Automation\n",
    "Which parts will you automate now? Which stay manual? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Write your rationale here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ab1c8",
   "metadata": {},
   "source": [
    "\n",
    "### Automate Now\n",
    "- **Ingest**: Reading raw CSVs is simple, deterministic, and should be scripted for reproducibility.  \n",
    "- **Clean**: Cleaning rules (null handling, type casting) are stable and benefit from automated checkpoints.  \n",
    "- **Feature Engineering**: Core features (`spend_income_ratio`, `rolling_spend_mean`) are consistent transformations, so automating ensures no manual mistakes.  \n",
    "- **Train Model**: Training with fixed parameters is deterministic and should always be automated to ensure reproducible model artifacts.  \n",
    "- **Evaluate**: Computing metrics (MAE, accuracy) is repeatable and should be logged automatically.\n",
    "\n",
    "### Keep Manual (for now)\n",
    "- **Report Writing**: While generating a basic metrics report can be automated, higher-level narrative and visual interpretation are better handled manually at this stage.  \n",
    "- **DAG Visualization**: The DAG structure may evolve, so maintaining a static diagram manually is sufficient for now.\n",
    "\n",
    "### Rationale\n",
    "- **Automated** steps are those that need to run consistently whenever new data arrives or experiments are rerun.  \n",
    "- **Manual** steps involve interpretation, storytelling, or visual presentation, which still benefit from human judgment and may change frequently.  \n",
    "- This balance allows us to focus engineering effort where reproducibility matters most (data → model → metrics), while keeping reporting flexible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) (Stretch) Refactor One Task into a Function + CLI\n",
    "Use the templates below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(input_path: str, output_path: str, log_path: str) -> None:\n",
    "    logging.info(\"[train_model] start\")\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Fill missing values with median\n",
    "    df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "    X = df[['income', 'transactions']].values\n",
    "    y = df['spend'].values\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    logging.info(\"[train_model] MAE = %.4f\", mae)\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    pickle.dump(model, open(output_path, \"wb\"))\n",
    "    logging.info(\"[train_model] model saved at %s\", output_path)\n",
    "\n",
    "    Path(log_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(f\"MAE: {mae:.4f}\\n\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Train model task\")\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Processed feature CSV\")\n",
    "    parser.add_argument(\"--output\", required=True, help=\"Path to save model.pkl\")\n",
    "    parser.add_argument(\"--log\", required=True, help=\"Path to save metrics log\")\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "    train_model(args.input, args.output, args.log)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example CLI call (for testing)\n",
    "    main([\n",
    "        \"--input\", \"../data/processed/features.csv\",\n",
    "        \"--output\", \"../models/model.pkl\",\n",
    "        \"--log\", \"../reports/metrics.txt\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Simple Retry Wrapper (fill in)\n",
    "Add a small retry with linear backoff to harden a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def retry(n_tries=3, delay=0.2):\n",
    "    def wrapper(fn, *args, **kwargs):\n",
    "        # TODO: implement try/except loop with sleep backoff\n",
    "        return fn(*args, **kwargs)\n",
    "    return wrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
